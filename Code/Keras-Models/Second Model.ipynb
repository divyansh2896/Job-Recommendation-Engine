{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import metrics\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.layers import Flatten\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, chunk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the entire dataset\n",
    "data = pd.read_csv('../../Results/JobsDataset.csv', header = 0, names = ['Query', 'Job Title', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data cleaning\n",
    "#Create Job description list\n",
    "job_descriptions=[]\n",
    "for job in data.Description:\n",
    "    j = job.replace(',', '')\n",
    "    job_descriptions.append(j)\n",
    "    \n",
    "#Words tokenization\n",
    "jobs = [word_tokenize(d) for d in job_descriptions]\n",
    "\n",
    "#Remove Capitalization\n",
    "no_capitals =[]\n",
    "for job in jobs:\n",
    "    no_capitals.append([j.lower() for j in job])\n",
    "\n",
    "#Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem=[]\n",
    "for job in no_capitals:\n",
    "    lem.append([lemmatizer.lemmatize(j) for j in job])\n",
    "\n",
    "#Remove stopwords\n",
    "filtered_words = []\n",
    "for job in lem:\n",
    "    filtered_words.append([j for j in job if not j in stopwords.words('english')])\n",
    "\n",
    "#Remove symbols\n",
    "cleaned_description=[]\n",
    "for job in filtered_words:\n",
    "    cleaned_description.append([j for j in job if not j in ['(',')','.',',',':','%']])\n",
    "\n",
    "#Final cleaned description list                                                            \n",
    "cleaned_desc=[]\n",
    "for description in cleaned_description:\n",
    "    cleaned_desc.append(\" \".join(description))\n",
    "\n",
    "                                                            \n",
    "#create new df \n",
    "df = pd.DataFrame({'Query':list(data.Query),'Description':cleaned_desc})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data to train and test (80 - 20)\n",
    "\n",
    "train, test = train_test_split(df, test_size = 0.2)\n",
    "\n",
    "train_descs = train['Description']\n",
    "train_labels = train['Query']\n",
    "#train_labels = train['Job Title']\n",
    " \n",
    "test_descs = test['Description']\n",
    "test_labels = test['Query']\n",
    "#test_labels = test['Job Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Parameters\n",
    "#Encoding\n",
    "vocab_size = 1000\n",
    "max_length = 1000\n",
    "\n",
    "#Model\n",
    "num_labels = 25\n",
    "embedding_dimensios = 15\n",
    "nb_epoch = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Training Data\n",
    "# Encode the jobs descriptions\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in train_descs]\n",
    "# pad documents to a max length\n",
    "x_train = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "#Binarize the job titles\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_labels)\n",
    "y_train = encoder.transform(train_labels)\n",
    "\n",
    "\n",
    "###Test Data\n",
    "# Encode the jobs descriptions\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in test_descs]\n",
    "# pad documents to a max length\n",
    "x_test = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "#Binarize the job titles\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(test_labels)\n",
    "y_test = encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 1000, 15)          15000     \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 15000)             0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 25)                375025    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 25)                0         \n",
      "=================================================================\n",
      "Total params: 390,025\n",
      "Trainable params: 390,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dimensios, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = 'adam', # or 'sgd'\n",
    "              metrics = [metrics.categorical_accuracy, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7200 samples, validate on 800 samples\n",
      "Epoch 1/30\n",
      "7200/7200 [==============================] - 6s 811us/step - loss: 2.8559 - categorical_accuracy: 0.1976 - acc: 0.1976 - val_loss: 2.2277 - val_categorical_accuracy: 0.3863 - val_acc: 0.3863\n",
      "Epoch 2/30\n",
      "7200/7200 [==============================] - 5s 710us/step - loss: 1.1271 - categorical_accuracy: 0.7654 - acc: 0.7654 - val_loss: 1.9449 - val_categorical_accuracy: 0.4075 - val_acc: 0.4075\n",
      "Epoch 3/30\n",
      "7200/7200 [==============================] - 5s 724us/step - loss: 0.3487 - categorical_accuracy: 0.9535 - acc: 0.9535 - val_loss: 2.0453 - val_categorical_accuracy: 0.4063 - val_acc: 0.4063\n",
      "Epoch 4/30\n",
      "7200/7200 [==============================] - 5s 718us/step - loss: 0.1695 - categorical_accuracy: 0.9846 - acc: 0.9846 - val_loss: 2.2074 - val_categorical_accuracy: 0.3950 - val_acc: 0.3950\n",
      "Epoch 5/30\n",
      "7200/7200 [==============================] - 5s 714us/step - loss: 0.1350 - categorical_accuracy: 0.9876 - acc: 0.9876 - val_loss: 2.2336 - val_categorical_accuracy: 0.4075 - val_acc: 0.4075\n",
      "Epoch 6/30\n",
      "7200/7200 [==============================] - 5s 718us/step - loss: 0.1169 - categorical_accuracy: 0.9885 - acc: 0.9885 - val_loss: 2.3745 - val_categorical_accuracy: 0.4025 - val_acc: 0.4025\n",
      "Epoch 7/30\n",
      "7200/7200 [==============================] - 5s 715us/step - loss: 0.1213 - categorical_accuracy: 0.9887 - acc: 0.9887 - val_loss: 2.4561 - val_categorical_accuracy: 0.3913 - val_acc: 0.3913\n",
      "Epoch 8/30\n",
      "7200/7200 [==============================] - 5s 711us/step - loss: 0.1160 - categorical_accuracy: 0.9899 - acc: 0.9899 - val_loss: 2.5162 - val_categorical_accuracy: 0.3975 - val_acc: 0.3975\n",
      "Epoch 9/30\n",
      "7200/7200 [==============================] - 5s 724us/step - loss: 0.1124 - categorical_accuracy: 0.9907 - acc: 0.9907 - val_loss: 2.5757 - val_categorical_accuracy: 0.3900 - val_acc: 0.3900\n",
      "Epoch 10/30\n",
      "7200/7200 [==============================] - 5s 722us/step - loss: 0.1135 - categorical_accuracy: 0.9906 - acc: 0.9906 - val_loss: 2.6653 - val_categorical_accuracy: 0.4025 - val_acc: 0.4025\n",
      "Epoch 11/30\n",
      "7200/7200 [==============================] - 5s 720us/step - loss: 0.1130 - categorical_accuracy: 0.9903 - acc: 0.9903 - val_loss: 2.7356 - val_categorical_accuracy: 0.3875 - val_acc: 0.3875\n",
      "Epoch 12/30\n",
      "7200/7200 [==============================] - 5s 713us/step - loss: 0.1136 - categorical_accuracy: 0.9901 - acc: 0.9901 - val_loss: 2.8263 - val_categorical_accuracy: 0.3750 - val_acc: 0.3750\n",
      "Epoch 13/30\n",
      "7200/7200 [==============================] - 5s 711us/step - loss: 0.1131 - categorical_accuracy: 0.9906 - acc: 0.9906 - val_loss: 2.8656 - val_categorical_accuracy: 0.3888 - val_acc: 0.3888\n",
      "Epoch 14/30\n",
      "7200/7200 [==============================] - 5s 720us/step - loss: 0.1144 - categorical_accuracy: 0.9907 - acc: 0.9907 - val_loss: 3.0316 - val_categorical_accuracy: 0.3688 - val_acc: 0.3688\n",
      "Epoch 15/30\n",
      "7200/7200 [==============================] - 5s 712us/step - loss: 0.1129 - categorical_accuracy: 0.9910 - acc: 0.9910 - val_loss: 3.0344 - val_categorical_accuracy: 0.3813 - val_acc: 0.3813\n",
      "Epoch 16/30\n",
      "7200/7200 [==============================] - 5s 711us/step - loss: 0.1111 - categorical_accuracy: 0.9903 - acc: 0.9903 - val_loss: 3.0827 - val_categorical_accuracy: 0.4013 - val_acc: 0.4013\n",
      "Epoch 17/30\n",
      "7200/7200 [==============================] - 5s 716us/step - loss: 0.1085 - categorical_accuracy: 0.9912 - acc: 0.9912 - val_loss: 3.1661 - val_categorical_accuracy: 0.3988 - val_acc: 0.3988\n",
      "Epoch 18/30\n",
      "7200/7200 [==============================] - 5s 714us/step - loss: 0.1051 - categorical_accuracy: 0.9914 - acc: 0.9914 - val_loss: 3.2841 - val_categorical_accuracy: 0.3925 - val_acc: 0.3925\n",
      "Epoch 19/30\n",
      "7200/7200 [==============================] - 5s 735us/step - loss: 0.1088 - categorical_accuracy: 0.9908 - acc: 0.9908 - val_loss: 3.3902 - val_categorical_accuracy: 0.3838 - val_acc: 0.3838\n",
      "Epoch 20/30\n",
      "7200/7200 [==============================] - 5s 708us/step - loss: 0.1086 - categorical_accuracy: 0.9910 - acc: 0.9910 - val_loss: 3.5128 - val_categorical_accuracy: 0.3750 - val_acc: 0.3750\n",
      "Epoch 21/30\n",
      "7200/7200 [==============================] - 5s 707us/step - loss: 0.1086 - categorical_accuracy: 0.9914 - acc: 0.9914 - val_loss: 3.5210 - val_categorical_accuracy: 0.3650 - val_acc: 0.3650\n",
      "Epoch 22/30\n",
      "7200/7200 [==============================] - 5s 715us/step - loss: 0.1057 - categorical_accuracy: 0.9917 - acc: 0.9917 - val_loss: 3.5910 - val_categorical_accuracy: 0.3675 - val_acc: 0.3675\n",
      "Epoch 23/30\n",
      "7200/7200 [==============================] - 5s 706us/step - loss: 0.1124 - categorical_accuracy: 0.9906 - acc: 0.9906 - val_loss: 3.5521 - val_categorical_accuracy: 0.3700 - val_acc: 0.3700\n",
      "Epoch 24/30\n",
      "7200/7200 [==============================] - 5s 706us/step - loss: 0.1059 - categorical_accuracy: 0.9912 - acc: 0.9912 - val_loss: 3.6140 - val_categorical_accuracy: 0.3725 - val_acc: 0.3725\n",
      "Epoch 25/30\n",
      "7200/7200 [==============================] - 5s 700us/step - loss: 0.1207 - categorical_accuracy: 0.9903 - acc: 0.9903 - val_loss: 3.6650 - val_categorical_accuracy: 0.3713 - val_acc: 0.3713\n",
      "Epoch 26/30\n",
      "7200/7200 [==============================] - 5s 703us/step - loss: 0.1200 - categorical_accuracy: 0.9907 - acc: 0.9907 - val_loss: 3.6886 - val_categorical_accuracy: 0.3750 - val_acc: 0.3750\n",
      "Epoch 27/30\n",
      "7200/7200 [==============================] - 5s 712us/step - loss: 0.1225 - categorical_accuracy: 0.9904 - acc: 0.9904 - val_loss: 3.9205 - val_categorical_accuracy: 0.3538 - val_acc: 0.3538\n",
      "Epoch 28/30\n",
      "7200/7200 [==============================] - 5s 714us/step - loss: 0.1220 - categorical_accuracy: 0.9901 - acc: 0.9901 - val_loss: 3.8841 - val_categorical_accuracy: 0.3663 - val_acc: 0.3663\n",
      "Epoch 29/30\n",
      "7200/7200 [==============================] - 5s 718us/step - loss: 0.1223 - categorical_accuracy: 0.9904 - acc: 0.9904 - val_loss: 4.0426 - val_categorical_accuracy: 0.3525 - val_acc: 0.3525\n",
      "Epoch 30/30\n",
      "7200/7200 [==============================] - 5s 722us/step - loss: 0.1208 - categorical_accuracy: 0.9910 - acc: 0.9910 - val_loss: 4.0264 - val_categorical_accuracy: 0.3513 - val_acc: 0.3513\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=nb_epoch,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 100us/step\n",
      "\n",
      "Test categorical_crossentropy: 4.1084576666355135\n",
      "Categorical accuracy: 0.32650000631809234\n",
      "Accuracy: 0.32650000631809234\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('\\nTest categorical_cro    ssentropy:', score[0])\n",
    "print('Categorical accuracy:', score[1])\n",
    "print('Accuracy:', score[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
