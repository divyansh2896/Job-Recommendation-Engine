{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geop\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from six.moves import urllib\n",
    "# import from nltk the functions that split a text into sentences and tokens\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from pprint import pprint\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import metrics\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, chunk\n",
    "from pprint import pprint\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the entire dataset\n",
    "data = pd.read_csv('../../Results/JobsDataset.csv', header = 0, names = ['Query', 'Job Title', 'Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Job description list\n",
    "job_descriptions=[]\n",
    "for job in data.Description:\n",
    "    j = job.replace(',', '')\n",
    "    job_descriptions.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Words tokenization\n",
    "jobs = [word_tokenize(d) for d in job_descriptions]\n",
    "\n",
    "#Remove Capitalization\n",
    "no_capitals =[]\n",
    "for job in jobs:\n",
    "    no_capitals.append([j.lower() for j in job])\n",
    "\n",
    "#Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lem=[]\n",
    "for job in no_capitals:\n",
    "    lem.append([lemmatizer.lemmatize(j) for j in job])\n",
    "\n",
    "#Remove stopwords\n",
    "filtered_words = []\n",
    "for job in lem:\n",
    "    filtered_words.append([j for j in job if not j in stopwords.words('english')])\n",
    "\n",
    "#Remove symbols\n",
    "cleaned_description=[]\n",
    "for job in filtered_words:\n",
    "    cleaned_description.append([j for j in job if not j in ['(',')','.',',',':','%']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geop\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "#Create model\n",
    "model = Word2Vec(cleaned_description , size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python [('scala', 0.8452833294868469), ('matlab', 0.8420097231864929), ('c++', 0.8312093019485474), ('java', 0.8310911655426025), ('ruby', 0.8222351670265198), ('perl', 0.795069694519043), ('bash', 0.7902927398681641), ('sa', 0.7801002264022827), ('panda', 0.7791467905044556), ('spss', 0.7786514163017273)]\n",
      "\n",
      "\n",
      "java [('php', 0.8766013383865356), ('javascript', 0.8724942207336426), ('c++', 0.8713992834091187), ('.net', 0.8709150552749634), ('node.js', 0.8591830730438232), ('scala', 0.85291588306427), ('nodejs', 0.8415697813034058), ('ruby', 0.8376085758209229), ('server-side', 0.8331729769706726), ('j2ee', 0.8321350812911987)]\n",
      "\n",
      "\n",
      "data [('datasets', 0.6018209457397461), ('output', 0.5738059282302856), ('metadata', 0.5628699660301208), ('predictive', 0.5558116436004639), ('extract', 0.5444742441177368), ('cleansing', 0.5444421172142029), ('etl', 0.5395274758338928), ('structured', 0.5234363079071045), ('directed/necessary', 0.5192700028419495), ('reporting', 0.5140564441680908)]\n",
      "\n",
      "\n",
      "c [('ee', 0.7918090224266052), ('asp.net', 0.7720319032669067), ('c++', 0.7337449789047241), ('j', 0.7118568420410156), ('.net', 0.6930859088897705), ('perl', 0.6902707815170288), ('batchfile', 0.6882214546203613), ('required/nice', 0.6779080629348755), ('mvc', 0.6771872639656067), ('vb.net', 0.6765270233154297)]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Skills Similarity\n",
    "for seed_word in [ 'python', 'java', 'data', 'c']:\n",
    "    print(seed_word,model.wv.most_similar(positive=[seed_word], topn=10))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
